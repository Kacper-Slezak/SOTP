[{"assignees":[],"body":"Create POST endpoint to receive data from SOTP Agents. Endpoint: POST /api/v1/ingest/metrics","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"}],"number":87,"state":"OPEN","title":"[BACKEND] Create Metrics Ingestion Endpoint"},{"assignees":[],"body":"Create a lightweight Python script in 'apps/sotp-agent' that collects Linux metrics (CPU/RAM) and Pushes data to POST /api/v1/ingest/metrics.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACU_nO0g","name":"area: agent","description":"Tasks related to SOTP Python Agent","color":"0E8A16"}],"number":86,"state":"OPEN","title":"[AGENT] Develop Python Metrics Agent"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"Move all services to 'apps/' directory. Separate Core API from Network Workers.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtf_wA","name":"refactor","description":"Code restructuring without behavioral changes","color":"FF99CC"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":85,"state":"OPEN","title":"[CHORE] Refactor: Restructure to Monorepo (apps/ folder)"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"### Goal\nStreamline network communication and resolve CORS issues by introducing an API gateway (Traefik) in front of containers.\n\n### Tasks\n- [ ] Add 'traefik' service to 'docker-compose.dev.yml'.\n- [ ] Configure routing:\n    - 'sotp.localhost/' -> directs to Frontend (port 3000)\n    - 'sotp.localhost/api' -> directs to Backend (port 8000)\n    - 'sotp.localhost/dashboard' -> directs to Traefik Dashboard (optional)\n- [ ] (Optional) Remove host port mapping for 3000 and 8000, keep only port 80/443 from Traefik.\n\n### Acceptance Criteria\n- Application is accessible under a single address (e.g., localhost or sotp.local).\n- Frontend communicates with API without CORS errors.","labels":[{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":82,"state":"OPEN","title":"[INFRA] Implement Traefik as Reverse Proxy (Docker Compose)"},{"assignees":[],"body":"$### Faza: 4.3\\n\\n**Opis:** Stworzenie strony `/reports` w interfejsie użytkownika.\\n\\n**Zadania:**\\n- [ ] Dodać formularz wyboru typu raportu i zakresu dat.\\n- [ ] Wywołać API i obsłużyć pobieranie wygenerowanego pliku.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMthJDg","name":"priority: low","description":"","color":"1d76dc"}],"number":79,"state":"CLOSED","title":"[FEAT] UI - Strona /reports do generowania raportów"},{"assignees":[],"body":"$### Faza: 4.3\\n\\n**Opis:** Stworzenie API do generowania raportów.\\n\\n**Zadania:**\\n- [ ] Stworzyć `ReportService` i endpoint `POST /api/v1/reports`.\\n- [ ] Dodać logikę agregującą dane z TimescaleDB (np. dostępność z 30 dni).\\n- [ ] Dodać generowanie plików PDF/CSV.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthJDg","name":"priority: low","description":"","color":"1d76dc"}],"number":78,"state":"CLOSED","title":"[FEAT] Implementacja API do generowania raportów (PDF/CSV)"},{"assignees":[],"body":"$### Faza: 4.2\\n\\n**Opis:** Stworzyć middleware FastAPI do automatycznego zapisu logów audytowych.\\n\\n**Zadania:**\\n- [ ] Middleware ma przechwytywać żądania `POST`, `PUT`, `DELETE`.\\n- [ ] Ma pobierać `user_id` z tokenu JWT.\\n- [ ] Zapisywać zdarzenie do tabeli `audit_logs`.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"}],"number":77,"state":"OPEN","title":"[FEAT] Middleware dla automatycznych logów audytowych"},{"assignees":[],"body":"$### Faza: 3.4\\n\\n**Opis:** Wdrożenie GitOps zgodnie z planem.\\n\\n**Zadania:**\\n- [ ] Stworzyć skrypty do instalacji ArgoCD na K3d.\\n- [ ] Stworzyć nowe repozytorium `sotp-k8s-config` i przenieść tam Helm chart.\\n- [ ] Zmodyfikować potok CD, aby aktualizował tag obrazu w nowym repo.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtf4Rw","name":"ci","description":"Change in pipeline CI/CD","color":"0052cc"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":76,"state":"CLOSED","title":"[INFRA] Konfiguracja ArgoCD i repozytorium sotp-k8s-config"},{"assignees":[],"body":"Create Helm charts for: apps/core-backend, apps/network-worker, apps/web-frontend","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":75,"state":"OPEN","title":"[INFRA] Create Helm Charts for Monorepo (Apps)"},{"assignees":[],"body":"$### Faza: 3.3\\n\\n**Opis:** Instrumentacja backendu (FastAPI, Celery, SQLAlchemy) za pomocą OpenTelemetry, aby wysyłać ślady do Tempo.\\n\\n**Zadania:**\\n- [ ] Dodać biblioteki `opentelemetry-*` do `requirements.txt`.\\n- [ ] Stworzyć `backend/app/core/observability.py`.\\n- [ ] Skonfigurować aplikację do wysyłania śladów do Tempo (OTLP).","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":74,"state":"OPEN","title":"[FEAT] Instrumentacja backendu (OpenTelemetry)"},{"assignees":[],"body":"$### Faza: 3.3\\n\\n**Opis:** Dodać serwis `tempo` do `docker-compose.dev.yml`.\\n\\n**Zadania:**\\n- [ ] Zaktualizować `datasource.yml` Grafany, aby dodać Tempo.\\n- [ ] Skonfigurować korelację śladów z logami (Tempo -> Loki).","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":73,"state":"OPEN","title":"[INFRA] Wdrożenie Grafana Tempo w Docker Compose"},{"assignees":[],"body":"$### Faza: 3.2\\n\\n**Opis:** Dodać serwisy `loki` i `promtail` do `docker-compose.dev.yml`.\\n\\n**Zadania:**\\n- [ ] Skonfigurować Promtail do automatycznego zbierania logów ze wszystkich kontenerów.\\n- [ ] Wysyłać logi do `http://loki:3100`.\\n- [ ] Zaktualizować `datasource.yml` Grafany, aby dodać Loki jako źródło danych.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":72,"state":"OPEN","title":"[INFRA] Wdrożenie Loki i Promtail w Docker Compose"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"### Goal\nEnable collection of technical metrics from Backend (response time, error codes) via Prometheus.\n\n### Tasks\n- [ ] Backend: Add 'prometheus-fastapi-instrumentator' library to 'requirements.txt'.\n- [ ] Backend: Configure the instrumentator in 'main.py' to expose the '/metrics' endpoint.\n- [ ] Infra: Update 'infrastructure/prometheus/prometheus.yml', changing target from 'localhost:9090' to 'backend:8000'.\n- [ ] Grafana: Create a simple dashboard to verify data flow (e.g., RPS).\n\n### Acceptance Criteria\n- Endpoint 'http://localhost:8000/metrics' returns data in Prometheus format.\n- Prometheus sees 'backend' target as UP.\n- Grafana charts show traffic after refreshing the page.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":63,"state":"OPEN","title":"[INFRA] Monitoring Implementation: Prometheus + FastAPI Instrumentator"},{"assignees":[],"body":"### Description\nSet up an End-to-End (E2E) testing framework (e.g., Playwright) and implement basic test scenarios covering critical user flows in the web interface.\n\n### Tasks\n- [ ] Add Playwright (`@playwright/test`) as a dev dependency to the `frontend` project.\n- [ ] Configure Playwright (e.g., `playwright.config.ts`) including base URL, browser settings, etc.\n- [ ] Write an E2E test script for the login flow:\n    - Navigate to `/login`.\n    - Fill in credentials.\n    - Submit form.\n    - Verify redirection to the dashboard and presence of an element indicating successful login.\n- [ ] Write an E2E test script for adding a new device:\n    - Log in (can use saved state or a fixture).\n    - Navigate to the devices page.\n    - Click \"Add Device\".\n    - Fill the device form.\n    - Submit form.\n    - Verify the new device appears in the list.\n- [ ] Write an E2E test script for viewing device details:\n    - Log in.\n    - Navigate to the devices page.\n    - Click on a device in the list.\n    - Verify navigation to the details page and presence of expected device information.\n- [ ] Integrate Playwright test execution into the CI pipeline (`.github/workflows/ci.yml`), potentially as a separate job that runs after build. Ensure services (backend, frontend, db) are running during the test.\n\n### Acceptance Criteria\n- E2E tests for login, adding a device, and viewing device details exist.\n- Tests run successfully against the application running in a test environment (e.g., Docker Compose).\n- E2E tests are executed automatically as part of the CI pipeline.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtf4Rw","name":"ci","description":"Change in pipeline CI/CD","color":"0052cc"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMthJDg","name":"priority: low","description":"","color":"1d76dc"},{"id":"LA_kwDOP-rqi88AAAACMtlyPw","name":"test","description":"","color":"ededed"}],"number":62,"state":"OPEN","title":"Tests - Configure and Implement E2E Tests"},{"assignees":[],"body":"### Description\nEnhance the JWT authentication system with features like token blacklisting (for immediate logout invalidation) and refresh token rotation for improved security.\n\n### Tasks\n- [ ] **Token Blacklisting:**\n    - Implement a mechanism to store invalidated tokens (e.g., using Redis with a TTL matching the token's remaining validity).\n    - Add a `POST /api/v1/auth/logout` endpoint that adds the current `access_token` (and potentially `refresh_token`) to the blacklist.\n    - Modify the token validation dependency to check the blacklist before accepting a token.\n- [ ] **Refresh Token Rotation:**\n    - Modify the `POST /api/v1/auth/refresh` endpoint:\n        - When a valid `refresh_token` is used, issue *both* a new `access_token` *and* a new `refresh_token`.\n        - Invalidate the used `refresh_token` (e.g., add it to a blacklist or use a database flag if storing refresh tokens).\n- [ ] Add tests for the logout endpoint and blacklist check.\n- [ ] Add tests for refresh token rotation (verifying the old refresh token is invalidated).\n\n### Acceptance Criteria\n- Logging out invalidates the current tokens immediately.\n- Using a refresh token invalidates it and issues a new one.\n- Security is enhanced by preventing reuse of logged-out or refreshed tokens.\n- Tests for new features pass.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthJDg","name":"priority: low","description":"","color":"1d76dc"}],"number":61,"state":"OPEN","title":"Backend - Implement Advanced JWT Features (Blacklisting, Rotation)"},{"assignees":[],"body":"### Description\nEnhance the SSH Command Runner to parse the raw output of commands using TextFSM templates, providing structured data in the API response.\n\n### Tasks\n- [ ] Add `textfsm` to `backend/requirements.txt`.\n- [ ] Create a directory for TextFSM templates, e.g., `backend/app/utils/textfsm_templates/`.\n- [ ] Obtain or create TextFSM templates for the whitelisted SSH commands (e.g., from `ntc-templates` or custom). Place them in the templates directory.\n- [ ] Create a utility function or class in `backend/app/utils/parsers.py` that takes raw command output and a command name (or template path) and returns parsed data using `textfsm`.\n- [ ] Modify `SshService.run_command` or the API endpoint `POST /devices/{id}/execute`:\n    - After getting the raw output, attempt to parse it using the appropriate TextFSM template.\n    - Include the parsed data (as a list of dictionaries) in the API response, e.g., `{\"output\": \"...\", \"parsed_data\": [...]}`. Handle cases where parsing fails or no template exists gracefully (e.g., `parsed_data: null`).\n- [ ] Update API tests to verify the presence and structure of `parsed_data` for commands with templates.\n\n### Acceptance Criteria\n- The API response for `/execute` includes a `parsed_data` field containing structured data when a corresponding TextFSM template exists and parsing is successful.\n- If no template exists or parsing fails, `parsed_data` is null or an empty list, and the raw `output` is still returned.\n- Tests verify the parsing functionality.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthJDg","name":"priority: low","description":"","color":"1d76dc"}],"number":60,"state":"OPEN","title":"Backend - Integrate TextFSM with SSH Command Runner"},{"assignees":[],"body":"### Description\nAdd integration tests for the `SnmpCollector` and `SshService` (including the API endpoint) to ensure they function correctly in scenarios closer to real-world usage.\n\n### Tasks\n- [ ] **SNMP Collector Tests:**\n    - Set up a way to mock an SNMP agent for tests (e.g., using a library like `pysnmp.testing` or potentially a dedicated Docker container running an SNMP simulator like `snmpsim`).\n    - Write integration tests for `SnmpCollector.collect` that:\n        - Verify correct fetching of basic OIDs (sysUpTime, sysDescr).\n        - Verify correct fetching of CPU, RAM metrics.\n        - Verify correct fetching and processing of interface table data (including bandwidth calculation if possible in the test setup).\n        - Test different scenarios: device unreachable, wrong community string.\n- [ ] **SSH Service/API Tests:**\n    - Set up a way to mock an SSH server for tests (e.g., using a library like `pytest-docker` to run a container with an SSH server, or a Python SSH server library like `paramiko` in testing mode).\n    - Write integration tests for the `POST /api/v1/devices/{id}/execute` endpoint that:\n        - Verify successful execution of an allowed command.\n        - Verify the correct output is returned.\n        - Verify an audit log entry is created.\n        - Test scenarios: disallowed command (expect 403/400), connection error, authentication error.\n- [ ] Ensure these tests can run reliably in the CI environment.\n\n### Acceptance Criteria\n- Integration tests for `SnmpCollector` cover fetching key metrics and handling common errors.\n- Integration tests for the SSH command execution endpoint cover successful execution, disallowed commands, audit logging, and error handling.\n- Tests pass successfully locally and in CI.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMtlyPw","name":"test","description":"","color":"ededed"}],"number":59,"state":"OPEN","title":"Tests - Add Integration Tests for SNMP and SSH Collectors"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"### Description\nCreate and configure Grafana dashboards (provisioned as code) to visualize ICMP and SNMP metrics collected by the SOTP platform and stored in Prometheus (requires metrics to be exported to Prometheus).\n\n### Tasks\n- [ ] **Requirement:** Ensure ICMP and SNMP metrics (or at least key ones like RTT, packet loss, CPU, RAM, basic interface stats) are being exported from the backend/collectors to Prometheus (this might require a separate issue/task, e.g., adding `prometheus-client` or using Pushgateway).\n- [ ] Update/Create the \"Device Details\" dashboard JSON (`infrastructure/grafana/dashboards/device-details.json`):\n    - Add panels for RTT, packet loss (from ICMP metrics in Prometheus).\n    - Add panels for CPU utilization, RAM utilization, System Uptime (from SNMP metrics in Prometheus).\n    - Add panels for Interface Bandwidth (In/Out) and Operational Status (using data from SNMP metrics). Use repeating panels if possible for interfaces.\n    - Configure a dashboard variable `$device` (e.g., based on a Prometheus label like `device_ip` or `device_name`) to select the device to display.\n- [ ] Create the \"Network Overview\" dashboard JSON (`infrastructure/grafana/dashboards/network-overview.json`):\n    - Add Stat panels for: Total Devices, Devices Up/Down (based on ICMP reachability).\n    - Add a Gauge panel for overall network health (e.g., % devices reachable).\n    - Add a Time series panel for average RTT across all devices.\n    - Add a Bar chart or Table for Top N devices by CPU utilization or latency.\n- [ ] Ensure dashboards are correctly provisioned when Grafana starts.\n- [ ] Document the dashboards and the required Prometheus metrics/labels in `docs/GRAFANA_SETUP.md`.\n\n### Acceptance Criteria\n- Grafana automatically loads \"Device Details\" and \"Network Overview\" dashboards.\n- The \"Device Details\" dashboard allows selecting a device and displays its ICMP and SNMP metrics from Prometheus.\n- The \"Network Overview\" dashboard provides a high-level summary of network health based on Prometheus data.\n- Dashboards are defined as JSON files in the infrastructure directory.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":58,"state":"OPEN","title":"Infra - Configure Grafana Dashboards (ICMP, SNMP)"},{"assignees":[],"body":"### Description\nIntegrate the \"Metrics\" tab on the device details page (`/devices/[id]`) with actual backend API endpoints to display historical metric data for that specific device.\n\n### Tasks\n- [ ] **Backend:**\n    - Create an API endpoint (e.g., `GET /api/v1/devices/{device_id}/metrics`) that accepts query parameters like `metric_name` (e.g., 'rtt_avg', 'cpu_utilization') and `time_range` (e.g., '1h', '24h', '7d').\n    - The endpoint should query the TimescaleDB `device_metrics` table for the specified device, metric, and time range.\n    - Return the data in a format suitable for charting (e.g., `[{ \"time\": \"...\", \"value\": ... }]`).\n    - Consider downsampling data for longer time ranges for performance.\n    - Secure the endpoint.\n- [ ] **Frontend:**\n    - On the `/devices/[id]` page, within the \"Metrics\" tab/section.\n    - Implement the time range selector component (buttons or dropdown).\n    - Use `react-query` (`useQuery`) to fetch data from the new backend endpoint based on the selected device ID, metric name, and time range.\n    - Pass the fetched data to the chart components (e.g., `recharts`) for ICMP (latency, packet loss) and SNMP (CPU, RAM, Bandwidth) metrics.\n    - Ensure charts update when the time range changes.\n    - Handle loading and error states for each chart.\n\n### Acceptance Criteria\n- User can select a time range (1h, 24h, 7d).\n- Charts for RTT, packet loss, CPU, RAM, and Interface Bandwidth display historical data fetched from the backend for the selected device and time range.\n- Charts update correctly when the time range is changed.\n- Loading and error states are handled.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"}],"number":57,"state":"OPEN","title":"Frontend - Implement Metrics Tab on Device Details Page (API Integration)"},{"assignees":[],"body":"### Description\nIntegrate the `/metrics` dashboard page with actual backend API endpoints to display aggregated data about system and device health. Requires creating corresponding endpoints in the backend.\n\n### Tasks\n- [ ] **Backend:**\n    - Create a new API router, e.g., `backend/app/api/v1/metrics.py`.\n    - Implement endpoint `GET /api/v1/metrics/summary` returning aggregated data, e.g.:\n        ```json\n        {\n          \"devices_total\": 100,\n          \"devices_online\": 95,\n          \"devices_offline\": 5,\n          \"avg_latency_ms\": 25.5,\n          \"alerts_active\": 3\n        }\n        ```\n        (This data might be calculated from recent readings in `device_metrics` or a separate status table).\n    - Implement endpoint `GET /api/v1/metrics/top_devices` returning e.g., top 5 devices by CPU usage or latency.\n    - Secure the new endpoints with appropriate roles.\n- [ ] **Frontend:**\n    - Update the `/metrics` page.\n    - Use `react-query` (`useQuery`) to fetch data from `/api/v1/metrics/summary` and `/api/v1/metrics/top_devices`.\n    - Display the fetched data in the \"card\" components and e.g., a simple table or bar chart for top devices.\n    - Add handling for loading and error states.\n    - Consider adding automatic data refresh every N seconds.\n\n### Acceptance Criteria\n- The `/metrics` dashboard displays real data fetched from the backend.\n- Summary statistics (total/online/offline devices, avg latency) are shown.\n- Top devices based on a selected metric are displayed.\n- Loading and error states are handled visually.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"}],"number":56,"state":"OPEN","title":"Frontend - Implement Basic Metrics Dashboard (API Integration)"},{"assignees":[],"body":"Move SSH logic to 'apps/network-worker'. API should only dispatch tasks.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACU_nOpA","name":"area: network-worker","description":"Tasks related to the standalone Network Worker service","color":"D93F0B"}],"number":55,"state":"OPEN","title":"[NETWORK-WORKER] Implement SSH Command Executor"},{"assignees":[],"body":"Implement CPU/RAM collection in the standalone 'apps/network-worker' service.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACU_nOpA","name":"area: network-worker","description":"Tasks related to the standalone Network Worker service","color":"D93F0B"}],"number":54,"state":"OPEN","title":"[NETWORK-WORKER] Enhance SNMP Collector (CPU/RAM)"},{"assignees":[],"body":"### Description\nWrite a suite of integration tests (API tests) to verify the correct functioning of the entire JWT authentication flow and the RBAC authorization mechanisms defined in previous tasks.\n\n### Tasks\n- [ ] **Registration:**\n    - Test successful registration of a new user.\n    - Test attempting to register with an existing email (expect 409 error).\n    - Test registration with invalid data (e.g., weak password, invalid email - expect 422 error).\n- [ ] **Login:**\n    - Test successful login with correct credentials (verify tokens are received).\n    - Test login with incorrect password (expect 401 error).\n    - Test login with non-existent user (expect 401/404 error).\n- [ ] **Token Refresh:**\n    - Test successfully refreshing `access_token` using a valid `refresh_token`.\n    - Test attempting to refresh with an invalid/expired `refresh_token` (expect 401 error).\n- [ ] **Endpoint Protection:**\n    - Test accessing a protected endpoint (e.g., `GET /devices`) with a valid `access_token`.\n    - Test accessing a protected endpoint without a token (expect 401 error).\n    - Test accessing a protected endpoint with an expired `access_token` (expect 401 error).\n    - Test accessing a protected endpoint with an invalid token (expect 401/403 error).\n- [ ] **RBAC:**\n    - Create test users with different roles (ADMIN, OPERATOR, READONLY).\n    - For each RBAC-protected endpoint (e.g., `DELETE /devices/{id}`), write tests:\n        - Checking access for a user with the required role (expect 2xx success).\n        - Checking lack of access for a user with insufficient role (expect 403 error).\n- [ ] Use `httpx.AsyncClient` to perform API requests in tests.\n\n### Acceptance Criteria\n- All defined test scenarios pass successfully.\n- Tests confirm that authentication and authorization work as expected.\n- Tests are part of the CI pipeline.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMtlyPw","name":"test","description":"","color":"ededed"}],"number":53,"state":"OPEN","title":"Tests - Add Integration Tests for Auth and RBAC"},{"assignees":[],"body":"### Description\nImplement global management of the user's session state (storing tokens, user data) and a mechanism to protect routes (pages) that should only be accessible to logged-in users.\n\n### Tasks\n- [ ] Configure a `zustand` store (e.g., `frontend/src/stores/authStore.ts`) to hold:\n    - `accessToken: string | null`\n    - `refreshToken: string | null`\n    - `user: User | null` (where `User` is a type for user data, e.g., id, email, role)\n    - `isAuthenticated: boolean`\n- [ ] Implement actions in the store: `login(tokens, userData)`, `logout()`, `setTokens(tokens)`.\n- [ ] Use `zustand/middleware/persist` to save the store's state (at least the tokens) to `localStorage` so the session survives page refresh.\n- [ ] Create a route protection mechanism:\n    - Option 1: Use Next.js Middleware (`middleware.ts`) to check the token (e.g., from cookie or localStorage via store) and redirect to `/login` if the user is not authenticated.\n    - Option 2: Create a Wrapper component (e.g., `AuthGuard`) in the main dashboard layout (`(dashboard)/layout.tsx`) that checks `isAuthenticated` from the store and redirects if `false`.\n- [ ] Apply protection to all routes within the `(dashboard)` group (e.g., `/devices`, `/metrics`).\n- [ ] Implement logic to refresh the `accessToken` using the `refreshToken`:\n    - This could be done in an `axios` interceptor - if an API request returns 401, try refreshing the token and retrying the request.\n    - Or on application initialization/page load if the `accessToken` is close to expiring.\n- [ ] Implement the `logout` function in `authStore` that clears the store, removes tokens from `localStorage`, and redirects to `/login`. Add a Logout button to the UI (e.g., in a user menu).\n\n### Acceptance Criteria\n- Attempting to access `/devices` as an unauthenticated user redirects to `/login`.\n- A logged-in user can access `/devices`.\n- Login state persists after refreshing the page.\n- Clicking \"Logout\" logs the user out and redirects to `/login`.\n- The system attempts to automatically refresh an expired `accessToken` using the `refreshToken`.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":52,"state":"OPEN","title":"Frontend - Implement Auth State Management and Route Protection"},{"assignees":[],"body":"### Description\nCreate the user interface for the login and registration processes, integrating them with the corresponding backend API endpoints.\n\n### Tasks\n- [ ] Create a new route group `(auth)` in `frontend/src/app`.\n- [ ] Create the `/login` page (`frontend/src/app/(auth)/login/page.tsx`).\n    - Add a form with fields: Email, Password.\n    - Use a form management library (e.g., `react-hook-form`) and validation (e.g., `zod`).\n    - Add a \"Login\" button.\n- [ ] Create the `/register` page (`frontend/src/app/(auth)/register/page.tsx`).\n    - Add a form with fields: Name, Email, Password, Confirm Password.\n    - Add validation.\n    - Add a \"Register\" button.\n- [ ] Implement logic to send form data to the API endpoints (`/auth/login`, `/auth/register`) using e.g., `axios` and `react-query` (mutations).\n- [ ] Handle loading states (e.g., disable button, show spinner).\n- [ ] Display error messages returned by the API (e.g., \"Invalid credentials\", \"User already exists\") using e.g., a Toast component.\n- [ ] On successful login/registration, store the received tokens (access and refresh) and user data.\n- [ ] Redirect the user to the main dashboard page (e.g., `/devices`).\n\n### Acceptance Criteria\n- User can enter data and submit the login form.\n- User can enter data and submit the registration form.\n- Client-side validation works correctly.\n- Communication with the backend API works (sending data, receiving responses).\n- API errors are displayed to the user.\n- On success, the user is redirected to the dashboard.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":51,"state":"OPEN","title":"Frontend - Create Login and Registration Pages"},{"assignees":[],"body":"Initialize the 'apps/network-worker' container/service structure.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACU_nOpA","name":"area: network-worker","description":"Tasks related to the standalone Network Worker service","color":"D93F0B"}],"number":50,"state":"OPEN","title":"[NETWORK-WORKER] Init SNMP Collector Service"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"### Description\nIntegrate the backend application with HashiCorp Vault for secure storage and retrieval of secrets, such as credentials for monitored devices (SNMP community, SSH passwords/keys).\n\n### Tasks\n- [ ] Add `hvac` to `backend/requirements.txt`.\n- [ ] Create `VaultService` in `backend/app/services/vault_service.py`.\n- [ ] Configure the `hvac` client within the service to connect to Vault (address and token obtained from application config - `app.core.config`).\n- [ ] Implement method `get_device_credentials(device_id: int, credential_type: str)`:\n    - Fetch the `Credential` object for the given `device_id` and `credential_type` from PostgreSQL.\n    - Use the `vault_path` from the `Credential` object to read the secret from Vault using `hvac`.\n    - Return the fetched credentials.\n- [ ] Implement method `set_device_credentials(device_id: int, credential_type: str, credentials: dict)`:\n    - Generate a unique Vault path (e.g., `secret/data/devices/{device_id}/{credential_type}`).\n    - Write the `credentials` to this path in Vault.\n    - Create or update the `Credential` object in PostgreSQL, saving the `vault_path`.\n- [ ] Modify the device management API endpoints (`POST` and `PUT /devices`) to accept credentials (e.g., SNMP community string) and use `VaultService.set_device_credentials` to store them in Vault. **Do not store secrets directly in the `devices` table!**\n- [ ] Future collectors (SNMP, SSH) should use `VaultService.get_device_credentials` to retrieve login details.\n- [ ] Add error handling (e.g., path not found in Vault, Vault connection error).\n- [ ] Write unit tests for `VaultService` (mocking the `hvac` client).\n- [ ] Write integration tests verifying secret storage and retrieval via the API (requires a running Vault instance in the test environment).\n\n### Acceptance Criteria\n- Device credentials are stored in Vault when creating/updating a device via the API.\n- Only the path to the secret in Vault is stored in the PostgreSQL database.\n- `VaultService` can retrieve credentials from Vault based on `device_id` and type.\n- Integration tests confirm correct secret writing and reading.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":49,"state":"OPEN","title":"Backend - Integrate Backend with HashiCorp Vault"},{"assignees":[],"body":"### Description\nIntroduce a Role-Based Access Control (RBAC) system. User roles (defined in the `User` model) should restrict access to specific API operations.\n\n### Tasks\n- [ ] Ensure the `User` model has a `role` field of Enum type (`ADMIN`, `OPERATOR`, `READONLY`, etc.).\n- [ ] Modify the JWT creation logic to include the user's role in the payload (`claims`).\n- [ ] Create a new FastAPI dependency (e.g., `require_role(allowed_roles: list[UserRole])`) that checks if the user's role (from the token) is in the list of allowed roles. If not, raise `HTTPException(403)`.\n- [ ] Apply `Depends(require_role(...))` to the device CRUD endpoints:\n    - `POST /devices`: e.g., `ADMIN`\n    - `GET /devices`, `GET /devices/{id}`: e.g., `ADMIN`, `OPERATOR`, `READONLY`\n    - `PUT /devices/{id}`: e.g., `ADMIN`, `OPERATOR`\n    - `DELETE /devices/{id}`: e.g., `ADMIN`\n- [ ] Create an endpoint (e.g., `GET /api/v1/users/me`) that returns the logged-in user's data (including role) for the frontend to adapt the UI.\n- [ ] Write API tests verifying that users with different roles only have access to their permitted endpoints/HTTP methods.\n\n### Acceptance Criteria\n- Access to API endpoints is restricted according to defined roles.\n- Attempting an unauthorized operation returns a 403 Forbidden error.\n- The `/users/me` endpoint returns correct data for the logged-in user.\n- API tests for RBAC pass.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":48,"state":"OPEN","title":"Backend - Implement Basic RBAC"},{"assignees":[],"body":"### Description\nImplement a JWT-based authentication system as specified in the project plan (Phase 2.1). This includes registration, login, token generation/validation (access & refresh), and securing endpoints.\n\n### Tasks\n- [ ] Create `POST /api/v1/auth/register` endpoint (with password hashing, e.g., `passlib`).\n- [ ] Create `POST /api/v1/auth/login` endpoint verifying password and returning `access_token` and `refresh_token`.\n- [ ] Implement functions in `app.core.security` to create and decode JWTs (using `python-jose`) with appropriate expiration times (e.g., access: 15min, refresh: 7d). Use a secret key from configuration.\n- [ ] Create `POST /api/v1/auth/refresh` endpoint accepting `refresh_token` and returning a new `access_token`.\n- [ ] Create a FastAPI dependency (`app.api.dependencies`) that verifies the `access_token` in the `Authorization: Bearer <token>` header and returns the user object (or raises `HTTPException`).\n- [ ] Apply this dependency to existing `/api/v1/devices` endpoints (all except perhaps health checks).\n- [ ] Add appropriate error handling (`HTTPException` with 401, 403 status codes).\n- [ ] Write unit tests for functions in `security.py`.\n- [ ] Write API (integration) tests for `/register`, `/login`, `/refresh` endpoints, and tests verifying the protection of `/devices`.\n\n### Acceptance Criteria\n- New users can register.\n- Registered users can log in and receive tokens.\n- Users can refresh their `access_token` using a `refresh_token`.\n- `/devices` endpoints are inaccessible without a valid `access_token` (return 401).\n- `/devices` endpoints work correctly with a valid `access_token`.\n- API tests for the authentication flow pass.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":47,"state":"OPEN","title":"Backend - Implement JWT Authentication"},{"assignees":[],"body":"### Description\nCreate a Python script to populate the database with sample data for development and demo purposes, and document the database schema.\n\n### Tasks\n- [ ] Create the script `scripts/seed_demo_data.py`.\n    - It should connect to the PostgreSQL database (using config from `app.core.config`).\n    - It should optionally clear existing data (with a flag).\n    - It should add a few users with different roles (Admin, Operator).\n    - It should add several example devices (different types, vendors, statuses).\n    - It should use SQLAlchemy models to create objects.\n- [ ] Add a command to the `Makefile` (e.g., `make seed`) to run this script (e.g., via `docker-compose exec backend python scripts/seed_demo_data.py`).\n- [ ] Generate a schema diagram for PostgreSQL and TimescaleDB tables (e.g., using an online tool like dbdiagram.io or an IDE feature).\n- [ ] Create the file `docs/DATABASE_SCHEMA.md`.\n    - Embed the generated diagram (as an image or mermaid code).\n    - Add descriptions for the main tables (users, devices, credentials, device_metrics), their key fields, and relationships.\n- [ ] Update `README.md` with instructions on how to use the `make seed` command.\n\n### Acceptance Criteria\n- Running `make seed` populates the database with sample data.\n- The file `docs/DATABASE_SCHEMA.md` exists and contains an up-to-date diagram and schema description.\n- `README.md` includes information about seeding data.","labels":[{"id":"LA_kwDOP-rqi88AAAACMdB9aw","name":"documentation","description":"Improvements or additions to documentation","color":"0075ca"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtgtqA","name":"area: docs","description":"","color":"d927fe"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":46,"state":"OPEN","title":"Docs - Create Seeding Script and Database Schema Documentation"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"### Description\nAdd Prometheus and Grafana services to the `docker-compose.dev.yml` file and configure their basic operation and connection.\n\n### Tasks\n- [x] Add `prometheus` and `grafana` service definitions to `infrastructure/docker/docker-compose.dev.yml`.\n    - Use official images.\n    - Map ports (e.g., 9090 for Prometheus, 3001 for Grafana - to avoid conflict with frontend on 3000).\n    - Configure volumes for data and configuration persistence.\n- [ ] Create `infrastructure/prometheus/prometheus.yml` with minimal global configuration and a `scrape_configs` section (can be empty or just scrape Prometheus itself for now). Mount this file into the Prometheus container.\n- [x] Create `infrastructure/grafana/datasources/datasources.yml` defining a Prometheus datasource pointing to the `prometheus:9090` service. Mount this file into the Grafana container in the appropriate provisioning path.\n- [x] Create the `infrastructure/grafana/dashboards/` directory.\n- [x] Create `infrastructure/grafana/dashboards/provider.yml` configuring dashboard provisioning from the `dashboards` folder.\n- [x] Add a simple example dashboard (e.g., monitoring Prometheus itself) as a JSON file in `infrastructure/grafana/dashboards/` to test provisioning.\n- [x] Update `README.md` or create `docs/MONITORING_SETUP.md` with information on accessing the Prometheus and Grafana UIs.\n\n### Acceptance Criteria\n- After running `make dev`, Prometheus and Grafana services start correctly.\n- Prometheus UI is accessible (e.g., at `localhost:9090`).\n- Grafana UI is accessible (e.g., at `localhost:3001`) and login works (default admin/admin).\n- The \"Prometheus\" datasource is automatically configured in Grafana.\n- The example dashboard is automatically imported into Grafana.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":45,"state":"CLOSED","title":"Infra - Configure Basic Monitoring Stack (Prometheus + Grafana)"},{"assignees":[{"id":"MDQ6VXNlcjU4MTA0OTMy","login":"kirilol-ok","name":"Kiryl Baranouski","databaseId":0}],"body":"### Description\nEnhance the `GET /api/v1/devices` endpoint to support pagination (`limit`, `offset`), sorting by specified fields (`sort_by`, `sort_order`), and filtering (e.g., `is_active`, `device_type`, `vendor`).\n\n### Tasks\n- [ ] Update `get_all_devices` (or the method in `DeviceService`) to accept query parameters: `limit: int = 10`, `offset: int = 0`, `sort_by: str = \"id\"`, `sort_order: str = \"asc\"`, `is_active: bool = None`, `device_type: str = None`, `vendor: str = None`.\n- [ ] Implement dynamic SQLAlchemy query building in the service based on the provided parameters.\n- [ ] Return the response in a format that includes metadata, e.g., `{ \"items\": [...], \"total_count\": ..., \"limit\": ..., \"offset\": ... }`.\n- [ ] Update the OpenAPI (Swagger) documentation for the endpoint.\n- [ ] Add API tests verifying pagination, sorting (various fields, asc/desc), and filtering (various criteria).\n\n### Acceptance Criteria\n- The number of results per page can be controlled (`limit`, `offset`).\n- The device list can be sorted by `id`, `name`, `ip_address` (ascending and descending).\n- The device list can be filtered by `is_active`, `device_type`, `vendor`.\n- The response includes pagination metadata.\n- API tests pass.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":44,"state":"OPEN","title":"Backend - Implement Pagination, Sorting, and Filtering for Device List"},{"assignees":[{"id":"MDQ6VXNlcjU4MTA0OTMy","login":"kirilol-ok","name":"Kiryl Baranouski","databaseId":0}],"body":"### Description\nMove the business logic (validation, database operations) from the device API endpoints (`main.py`) to a dedicated `DeviceService` class in `backend/app/services/device_service.py`.\n\n### Tasks\n- [x] Create the file `backend/app/services/device_service.py`.\n- [x] Implement a `DeviceService` class with methods like `create_device`, `get_device_by_id`, `get_all_devices`, `update_device`, `delete_device`.\n- [x] Move the logic from the FastAPI handlers (`main.py`) into the corresponding service methods.\n- [x] Update the FastAPI handlers to use `DeviceService` (via Dependency Injection).\n- [ ] Write unit tests for `DeviceService` (mocking the DB session).\n\n### Acceptance Criteria\n- Business logic is separated from the API layer into `DeviceService`.\n- API endpoints utilize `DeviceService`.\n- Unit tests for `DeviceService` pass.\n- Existing CRUD functionality works as before.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtf_wA","name":"refactor","description":"Code restructuring without behavioral changes","color":"FF99CC"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":43,"state":"OPEN","title":"Backend - Refactor CRUD Logic to Service Layer"},{"assignees":[],"body":"### Description\nModify the `DELETE /api/v1/devices/{id}` endpoint to perform a \"soft delete\" (setting the `deleted_at` field) instead of removing the record from the database. The `GET /api/v1/devices` endpoint should, by default, only return active devices (where `deleted_at IS NULL`).\n\n### Tasks\n- [ ] Update the logic in the `delete_device` handler in `main.py`.\n- [ ] Modify the query in `get_all_devices` to filter by `deleted_at IS NULL`.\n- [ ] Add tests to verify the soft delete functionality.\n\n### Acceptance Criteria\n- Deleting a device via the API sets the `deleted_at` timestamp.\n- The device list (`GET /api/v1/devices`) does not show soft-deleted devices by default.\n- Tests for soft delete pass.","labels":[{"id":"LA_kwDOP-rqi88AAAACMdB9Yg","name":"fix","description":"Something isn't working","color":"d73a4a"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":42,"state":"OPEN","title":"Backend - Implement Soft Delete for Devices"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"Our current CI pipeline in `.github/workflows/ci.yml` provides a solid foundation for linting, testing, and building our application. However, to improve our development workflow and increase the reliability and security of our codebase, several enhancements should be implemented.\n\nThis task focuses on maturing our CI process by adding automated security scanning and stricter code quality enforcement.\n\n### **Is your feature request related to a problem?**\n\n1.  **Security Vulnerabilities**: We currently don't have any automated checks for security vulnerabilities in our Python dependencies (`safety`) or static code analysis (`bandit`). This exposes us to potential risks that could be caught early.\n2.  **Inconsistent Code Quality**: Although we have a `.pre-commit-config.yaml` file, its checks are not enforced by the CI pipeline. This means a developer could accidentally commit code that doesn't follow our style guidelines.\n3.  **Disabled Frontend Linting**: The frontend linting job is currently disabled, leading to a gap in our quality assurance for the Next.js application.\n4.  **Slow Feedback Loop**: The current jobs run sequentially (`lint` -> `test`). Some of these checks could run in parallel to provide faster feedback to developers.\n\n### **Describe the Solution You'd Like**\n\nThe `ci.yml` workflow should be updated to include the following improvements:\n\n1.  **Add a `quality` Job**:\n    * This new job should run `pre-commit run --all-files` to enforce all configured checks (`black`, `isort`, `end-of-file-fixer`, etc.) on the entire codebase.\n    * This job should run in parallel with the `test` and `security` jobs.\n\n2.  **Add a `security` Job**:\n    * This job will be responsible for all security-related scans.\n    * **Backend**:\n        * Run `bandit -r backend/app` for static application security testing (SAST).\n        * Run `safety check -r backend/requirements.txt` to check for known vulnerabilities in Python dependencies.\n    * **Frontend**:\n        * Run `npm audit` to check for vulnerabilities in Node.js dependencies.\n\n3.  **Update the `test` Job**:\n    * Re-enable the `npm run lint` step for the frontend to ensure code quality is checked on every commit.\n    * Make this job dependent on the `quality` and `security` jobs, so it runs only if they pass.\n\n4.  **Optimize Workflow Structure**:\n    * Refactor the workflow to allow `quality`, `security`, and `test` jobs to run in parallel to speed up the pipeline.\n    * The `build` job should depend on the successful completion of all three.\n\n### **Acceptance Criteria**\n\n* [ ] A new `quality` job is added to the CI workflow and successfully runs `pre-commit`.\n* [ ] A new `security` job is added, which performs scans with `bandit`, `safety`, and `npm audit`.\n* [ ] The pipeline fails if any of the security scans detect critical vulnerabilities.\n* [ ] The frontend linting step (`npm run lint`) is re-enabled and successfully runs within the `test` job.\n* [ ] The `quality`, `security`, and `test` jobs are configured to run in parallel.\n* [ ] The final `build` job only starts after `quality`, `security`, and `test` have all passed successfully.\n* [ ] The `notify_on_push` job correctly reflects the overall status of all preceding jobs.","labels":[],"number":35,"state":"CLOSED","title":"[FEAT] Enhance CI/CD Pipeline with Security Scanning and Quality Gates"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"Add Celery and Vault to docker-compose.dev.yml \nAdd healtchecks for all database and slow running services\nOptimalize docker-compse files","labels":[],"number":29,"state":"CLOSED","title":"[FEAT] All services in docker-compse.dev.yml with healthchecks"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"Develop README.md file to become a best short docs for a project\n\n<!-- Edit the body of your new issue then click the ✓ \"Create Issue\" button in the top right of the editor. The first line will be the issue title. Assignees and Labels follow after a blank line. Leave an empty line before beginning the body of the issue. -->","labels":[{"id":"LA_kwDOP-rqi88AAAACMdB9aw","name":"documentation","description":"Improvements or additions to documentation","color":"0075ca"},{"id":"LA_kwDOP-rqi88AAAACMtgtqA","name":"area: docs","description":"","color":"d927fe"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"}],"number":23,"state":"CLOSED","title":"[DOCS]  README.md file "},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nAs per task 1.7 from the Master Document, the current `ci.yml` workflow needs to be enhanced to run actual tests for both backend and frontend, instead of just simulating the process.\n\n**Acceptance Criteria**\n- [ ] The `ci.yml` workflow is modified to include separate steps for running backend and frontend tests.\n- [ ] A new step is added that executes `pytest` within the `backend` directory.\n- [ ] A new step is added that executes `npm test` (or `npm run test`) within the `frontend` directory.\n- [ ] The workflow must **fail** if any tests fail.\n- [ ] The workflow should **pass** if tests are successful, or if no tests are found by the test runners (`pytest`, `vitest`).\n- [ ] The old \"Simulate build and tests\" step is removed.\n- [ ] The purpose of the `test_discord.yml` file is reviewed, and the file is either removed or documented as a utility script.\n\n**Proposed `ci.yml` implementation:**\n```yaml\n# .github/workflows/ci.yml - Główny workflow CI (Lint, Test, Build)","labels":[{"id":"LA_kwDOP-rqi88AAAACMtf4Rw","name":"ci","description":"Change in pipeline CI/CD","color":"0052cc"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":19,"state":"CLOSED","title":"[CI] Enhance CI workflow with dynamic test execution"},{"assignees":[],"body":"**Task Goal**\nAs per task 1.7 in the Master Document, write tests for the main UI components related to device management.\n\n**Acceptance Criteria**\n- [ ] **Component Tests:** The Device Table component is tested to ensure it correctly renders data passed via props.\n- [ ] **Component Tests:** The Device Form is tested to ensure validation works and the submission handler is called correctly.\n- [ ] **Integration Tests (React Testing Library):** A test is written that simulates a user filling out the form, submitting it, and verifying that the device list is updated.\n- [ ] All critical user interactions on the device pages are covered by tests.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMtlyPw","name":"test","description":"","color":"ededed"}],"number":18,"state":"OPEN","title":"[TEST] Write unit and component tests for Device UI"},{"assignees":[{"id":"MDQ6VXNlcjU4MTA0OTMy","login":"kirilol-ok","name":"Kiryl Baranouski","databaseId":0}],"body":"**Task Goal**\nAs per task 1.7 in the Master Document, write comprehensive tests for the Device CRUD functionality implemented in Issues #10.\n\n**Acceptance Criteria**\n- [ ] **Unit Tests:** Business logic in `device_service.py` is tested in isolation (using mocks for the database).\n- [ ] **API Tests:** Each CRUD endpoint (`POST`, `GET`, `PUT`, `DELETE` for `/api/v1/devices`) is tested using an async HTTP client like `httpx`.\n- [ ] **Integration Tests:** Tests verify that the API correctly interacts with a real test database.\n- [ ] Test coverage for the device-related modules is above 80%.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMtlyPw","name":"test","description":"","color":"ededed"}],"number":17,"state":"OPEN","title":"[TEST] Write unit and integration tests for Device CRUD API"},{"assignees":[],"body":"**Task Goal**\nCreate a form (in a modal or on a separate page) for creating and editing devices.\n\n**Acceptance Criteria**\n- [ ] A form is created with fields: Name, IP Address, Type, Vendor, Model, Location.\n- [ ] Client-side validation is implemented (e.g., checking IP address format).\n- [ ] On successful form submission, data is sent to the API (`POST` or `PUT /api/v1/devices`).\n- [ ] The user is notified of success or failure via toast notifications.\n- [ ] After adding a new device, the list on the `/devices` page is automatically refreshed.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":16,"state":"OPEN","title":"[FEAT] UI - Form for adding and editing devices"},{"assignees":[{"id":"U_kgDOCu_nZA","login":"Martynka123-code","name":"Martyna","databaseId":0}],"body":"**Task Goal**\nCreate the `/devices` page that displays a list of devices fetched from the API.\n\n**Acceptance Criteria**\n- [x] A new page is created at the `/devices` route.\n- [ ] A table component (e.g., using `shadcn/ui`) is implemented with columns: Name, IP Address, Type, Status, Actions.\n- [ ] React Query is integrated to fetch data from the `GET /api/v1/devices` endpoint.\n- [ ] Loading state (spinner) and error state (error message) are handled.\n- [ ] An \"Add Device\" button is present, which will navigate to the form.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":15,"state":"OPEN","title":"[FEAT] UI - Table for displaying device list"},{"assignees":[{"id":"U_kgDOCu_nZA","login":"Martynka123-code","name":"Martyna","databaseId":0}],"body":"**Task Goal**\nImplement the main UI frame, including the navigation bar and sidebar menu.\n\n**Acceptance Criteria**\n- [x] The main `layout.tsx` file is created.\n- [x] A persistent Sidebar component is implemented with links to: Devices, Metrics, Logs, Alerts, Settings.\n- [x] A top Navbar component is implemented with the logo and a placeholder for the user menu.\n- [x] The layout is responsive and displays correctly on mobile devices.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":14,"state":"CLOSED","title":"[FEAT] Create main application layout and navigation"},{"assignees":[{"id":"U_kgDOCu_nZA","login":"Martynka123-code","name":"Martyna","databaseId":0}],"body":"**Task Goal**\nAs per task 1.6 in the Master Document, create and configure the frontend project from scratch.\n\n**Acceptance Criteria**\n- [x] A new Next.js 14+ project with App Router is initialized.\n- [x] TypeScript is configured in `strict` mode.\n- [ ] Tailwind CSS is integrated and configured.\n- [x] Core libraries are added: `axios`, `@tanstack/react-query`, `zustand`.\n- [x] The application runs correctly inside the `frontend` container.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgkQg","name":"area: frontend","description":"","color":"de722b"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":13,"state":"CLOSED","title":"[FEAT] Initialize Next.js project with Tailwind CSS and TypeScript"},{"assignees":[{"id":"U_kgDOCe8ISA","login":"damian677","name":"","databaseId":0}],"body":"**Task Goal**\nAs per task 1.5 in the Master Document, implement a monitoring mechanism for devices using the ICMP (ping) protocol.\n\n**Acceptance Criteria**\n- [ ] A Celery task (`poll_device_icmp`) is created to perform an asynchronous ping to a given device.\n- [ ] Celery Beat is configured to periodically run this task for all active devices.\n- [ ] The results (RTT, packet loss) are correctly saved to the TimescaleDB database.\n- [ ] Basic error handling (e.g., timeout) is implemented.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgzxw","name":"priority: critical","description":"","color":"b60205"}],"number":12,"state":"CLOSED","title":"[FEAT] Implement ICMP collector as a Celery task"},{"assignees":[{"id":"MDQ6VXNlcjU4MTA0OTMy","login":"kirilol-ok","name":"Kiryl Baranouski","databaseId":0}],"body":"**Task Goal**\nAs per task 1.4 in the Master Document, implement the full set of API endpoints for managing devices.\n\n**Acceptance Criteria**\n- [x] `POST /api/v1/devices` endpoint is created to add a new device.\n- [x] `GET /api/v1/devices` endpoint is created to fetch a list of devices (with pagination).\n- [x] `GET /api/v1/devices/{id}` endpoint is created to fetch details of a single device.\n- [x] `PUT /api/v1/devices/{id}` endpoint is created to update a device.\n- [x] `DELETE /api/v1/devices/{id}` endpoint is created for soft-deleting a device.\n- [x] Appropriate Pydantic schemas and service layer logic are implemented.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":11,"state":"CLOSED","title":"[FEAT] Implement API for device management (CRUD)"},{"assignees":[{"id":"MDQ6VXNlcjU4MTA0OTMy","login":"kirilol-ok","name":"Kiryl Baranouski","databaseId":0}],"body":"**Task Goal**\nAs per task 1.3 in the Master Document, create the foundational skeleton of the FastAPI application from scratch.\n\n**Acceptance Criteria**\n- [x] The main application file `backend/app/main.py` is created.\n- [x] The application successfully connects to the PostgreSQL and Redis databases provided by Docker Compose.\n- [x] Basic middleware (e.g., CORS) is configured.\n- [x] A `/health` endpoint is created that returns a `{\"status\": \"ok\"}` status.\n- [x] The basic directory structure for `api`, `services`, and `schemas` is prepared for future development.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":10,"state":"CLOSED","title":"[FEAT] Initialize and configure FastAPI application skeleton"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nSet up the Alembic tool to manage schema changes for the PostgreSQL database.\n\n**Acceptance Criteria**\n- [ ] Alembic is configured in the `backend/alembic` directory.\n- [ ] A script or a `Makefile` command (e.g., `make migrate`) is created to run migrations inside the container.\n- [ ] The first migration is successfully generated based on the SQLAlchemy models from the previous task.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":9,"state":"CLOSED","title":"[DB] Configure Alembic and generate the initial migration"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nAs per task 1.2 in the Master Document, implement all ORM model classes in Python that reflect the designed database structure.\n\n**Acceptance Criteria**\n- [x] All required model classes are implemented in Python under `backend/app/models/` using SQLAlchemy ORM.\n- [x] All relationships, indexes, and constraints are correctly defined in the models.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgh0Q","name":"area: backend","description":"","color":"d4c5f9"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"}],"number":8,"state":"CLOSED","title":"[DB] Create SQLAlchemy models for all tables"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nCreate an initialization script that automatically prepares TimescaleDB for storing metrics.\n\n**Acceptance Criteria**\n- [ ] An `init-scripts/timescale-init.sql` file is created.\n- [ ] The script contains SQL commands to automatically convert the `device_metrics` table into a `hypertable`, which is critical for performance.\n- [ ] The script is correctly mounted and executed by the `timescaledb` service on its first run.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtfgOw","name":"feat","description":"Feature like new functions","color":"5319e7"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"}],"number":7,"state":"CLOSED","title":"[DB] Create SQL script to initialize hypertable in TimescaleDB"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nProvide a stable TimescaleDB database, optimized for storing time-series metrics.\n\n**Acceptance Criteria**\n- [ ] The `timescaledb` service is defined in the `docker-compose` files.\n- [ ] Data is stored in a persistent named volume (`timescale_data`).\n- [ ] Credentials are loaded from environment variables.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":6,"state":"CLOSED","title":"DB] Configure and set up the TimescaleDB service"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nProvide a stable PostgreSQL database for storing relational data such as users and devices.\n\n**Acceptance Criteria**\n- [ ] The `postgres` service is defined in the `docker-compose` files.\n- [ ] Data is stored in a persistent named volume (`postgres_data`) that survives container restarts.\n- [ ] Credentials (user, password, db name) are loaded from environment variables via the `.env` file.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtgqgQ","name":"area: database","description":"","color":"c1a10b"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":5,"state":"CLOSED","title":"[DB] Configure and set up the PostgreSQL service"},{"assignees":[],"body":"**Task Goal**\nEnsure secure and efficient communication between all containers in the stack.\n\n**Acceptance Criteria**\n- [x] A main `bridge` network named `sotp_network` is defined.\n- [x] All services are attached to this network.\n- [x] Services that do not need to be exposed externally (e.g., databases) only have their ports mapped in the `dev.yml` file for development purposes.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMthC9w","name":"priority: medium","description":"","color":"d93f0b"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":4,"state":"CLOSED","title":"[BUILD] Define networking and communication rules in Docker Compose"},{"assignees":[],"body":"**Task Goal**\nAdapt the existing `docker-compose.dev.yml` file to *extend*, rather than replace, the production configuration. It should only contain development-specific overrides.\n\n**Acceptance Criteria**\n- [ ] The `docker-compose.dev.yml` file now only contains sections that differ from production (e.g., source code `volumes`, the `devops` service, port mappings).\n- [ ] Running `docker-compose -f docker-compose.prod.yml -f docker-compose.dev.yml up` correctly merges both configurations.\n- [ ] The `Makefile` and `.devcontainer` files are updated to use the new, combined configuration.","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":3,"state":"CLOSED","title":"[BUILD] Refactor docker-compose.dev.yml as an override file"},{"assignees":[{"id":"U_kgDOCMrUyA","login":"Kacper-Slezak","name":"Kacper Slezak","databaseId":0}],"body":"**Task Goal**\nPhase 1.1 taks in Master Document, creat the main 'docker-compose.prod.yml' that defines the production application architecture.\n\n**Acceptance Criteria**\n- [ ] The file `infrastructure/docker/docker-compose.prod.yml` is created.\n- [ ] All services from the plan are defined (Traefik, FastAPI, Celery, databases, monitoring, etc.).\n- [ ] Images for `backend` and `frontend` are built from their `Dockerfile` without mounting source code volumes.\n- [ ] Persistent named volumes are defined for all data that must survive container restarts.\n\n\n<!-- Edit the body of your new issue then click the ✓ \"Create Issue\" button in the top right of the editor. The first line will be the issue title. Assignees and Labels follow after a blank line. Leave an empty line before beginning the body of the issue. -->","labels":[{"id":"LA_kwDOP-rqi88AAAACMtgEtQ","name":"build","description":"Changes in Docker, infrastructure, dependecies","color":"fbca04"},{"id":"LA_kwDOP-rqi88AAAACMtg-tQ","name":"priority: high","description":"","color":"b60108"},{"id":"LA_kwDOP-rqi88AAAACMthb2Q","name":"area: infrastructure","description":"","color":"ededed"}],"number":2,"state":"CLOSED","title":"[BUILD] Create base production docker-compose.prod.yml file"}]
